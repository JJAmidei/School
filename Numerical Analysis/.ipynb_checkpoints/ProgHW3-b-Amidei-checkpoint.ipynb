{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87c15c0c",
   "metadata": {},
   "source": [
    "# Problem 1\n",
    "Use the Jacobi Method to solve the following system for $n = 100$. Use the stopping criterion that the infinity norm of the difference between the iterate and true solution is less than $10^{-6}$ or the number of iterations reaches $n$. The correct solution is $[1,\\dots,1]$, compute the right-hand side $b$ using the np.dot function to multiply matrix and vector. Report the number of steps needed and the forward error (difference from the solution) and the backward error (the residual) in the infinity norm. The system is\n",
    "\\begin{equation*}\n",
    "\\left[\\begin{array}{rcccc}{3} & {-1} & {} & {} & {} \\\\ {-1} & {3} & {-1} & {} & {} \\\\ {} & {\\ddots} & {\\ddots} & {\\ddots} & {} \\\\ {} & {} & {-1} & {3} & {-1} \\\\ {} & {} & {} & {-1} & {3}\\end{array}\\right]\\left[\\begin{array}{c}{x_{1}} \\\\ {\\vdots} \\\\ {x_{n}}\\end{array}\\right]=b\n",
    "\\end{equation*}\n",
    "**MATH 5660 only:**\n",
    "\n",
    "(a) Same as above, but do not store the matrix $A$. This is possible since you know what its entries are and it has a special structure so you can just hardcode them and compute $b[i]$ and new $x[i]$ by formulas. $Make sure you get the same result as above.\n",
    "\n",
    "(b) Change the diagonal entries of $A$ to $2$, compute new $b$, and run your code for $n=100, 1000, 10000$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0aff839d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A = \n",
      " [[ 3. -1.  0. ...  0.  0.  0.]\n",
      " [-1.  3. -1. ...  0.  0.  0.]\n",
      " [ 0. -1.  3. ...  0.  0.  0.]\n",
      " ...\n",
      " [ 0.  0.  0. ...  3. -1.  0.]\n",
      " [ 0.  0.  0. ... -1.  3. -1.]\n",
      " [ 0.  0.  0. ...  0. -1.  3.]]\n",
      "\n",
      "b (computed using np.dot(A, b)) = \n",
      " [2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 2.]\n",
      "\n",
      "x = \n",
      " [0.99999968 0.99999937 0.99999907 0.99999881 0.99999856 0.99999837\n",
      " 0.99999818 0.99999806 0.99999794 0.99999787 0.9999978  0.99999776\n",
      " 0.99999773 0.99999771 0.9999977  0.99999769 0.99999769 0.99999769\n",
      " 0.99999768 0.99999768 0.99999768 0.99999768 0.99999768 0.99999768\n",
      " 0.99999768 0.99999768 0.99999768 0.99999768 0.99999768 0.99999768\n",
      " 0.99999768 0.99999768 0.99999768 0.99999768 0.99999768 0.99999768\n",
      " 0.99999768 0.99999768 0.99999768 0.99999768 0.99999768 0.99999768\n",
      " 0.99999768 0.99999768 0.99999768 0.99999768 0.99999768 0.99999768\n",
      " 0.99999768 0.99999768 0.99999768 0.99999768 0.99999768 0.99999768\n",
      " 0.99999768 0.99999768 0.99999768 0.99999768 0.99999768 0.99999768\n",
      " 0.99999768 0.99999768 0.99999768 0.99999768 0.99999768 0.99999768\n",
      " 0.99999768 0.99999768 0.99999768 0.99999768 0.99999768 0.99999768\n",
      " 0.99999768 0.99999768 0.99999768 0.99999768 0.99999768 0.99999768\n",
      " 0.99999768 0.99999768 0.99999768 0.99999768 0.99999769 0.99999769\n",
      " 0.99999769 0.9999977  0.99999771 0.99999773 0.99999776 0.9999978\n",
      " 0.99999787 0.99999794 0.99999806 0.99999818 0.99999837 0.99999856\n",
      " 0.99999881 0.99999907 0.99999937 0.99999968]\n",
      "\n",
      "Iterations: 32\n",
      "\n",
      "Forward error: 7.726460993229267e-07\n",
      "\n",
      "Backward error: 1.5452818485917064e-06\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def jacobi(A, b, x0, eps=1e-6, max_iterations=None):\n",
    "    d = np.diag(A)\n",
    "    r = A - np.diag(d)\n",
    "    x = x0.copy()\n",
    "    k = 0\n",
    "\n",
    "    while True:\n",
    "        x_2 = (b - np.dot(r, x)) / d\n",
    "        ferror = np.linalg.norm(x_2 - x, np.inf)\n",
    "        berror = np.linalg.norm(b - np.dot(A, x_2), np.inf)\n",
    "\n",
    "        if ferror < eps or (max_iterations is not None and k >= max_iterations):\n",
    "            break\n",
    "\n",
    "        x = x_2\n",
    "        k += 1\n",
    "\n",
    "    return x, k, ferror, berror\n",
    "\n",
    "n = 100\n",
    "diag = 3 * np.ones(n)\n",
    "offdiag = -np.ones(n - 1)\n",
    "A = np.diag(diag) + np.diag(offdiag, k=-1) + np.diag(offdiag, k=1)\n",
    "\n",
    "correct_solution = np.ones(n)\n",
    "\n",
    "b = np.dot(A, correct_solution)\n",
    "\n",
    "x0 = np.zeros(n)\n",
    "\n",
    "x, iterations, ferror, berror = jacobi(A, b, x0)\n",
    "\n",
    "print('A = \\n', A)\n",
    "print()\n",
    "print('b (computed using np.dot(A, b)) = \\n', b)\n",
    "print()\n",
    "print('x = \\n', x)\n",
    "print()\n",
    "print('Iterations:', iterations)\n",
    "print()\n",
    "print('Forward error:', ferror)\n",
    "print()\n",
    "print('Backward error:', berror)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc855125",
   "metadata": {},
   "source": [
    "# Problem 2\n",
    "Carry out the steps of Problem 1 with $n = 100$ for \n",
    "\n",
    "(a) Gauss–Seidel Method and\n",
    "\n",
    "(b) SOR with $\\omega = 1.2$.\n",
    "\n",
    "Which one converges faster - Jacobi, Gauss-Seidel, or SOR?\n",
    "\n",
    "**MATH 5660 only:**\n",
    "\n",
    "Carry out the steps as in Problem 1 part for MATH 5660 only, with the Gauss–Seidel Method and SOR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd4cff73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With Gauss_Seidel Method:\n",
      "\n",
      "Iterations: 19\n",
      "\n",
      "Forward error: 9.5367431640625e-07\n",
      "\n",
      "Backward error: 9.564705892861625e-07\n"
     ]
    }
   ],
   "source": [
    "# GS Method\n",
    "\n",
    "def Gauss_Seidel(A, b, x0, eps=1e-6, max_iterations=None):\n",
    "    n = A.shape[0]\n",
    "    x = x_0.copy()\n",
    "    k = 0\n",
    "\n",
    "    while True:\n",
    "        for i in range(n):\n",
    "            sum = 0.\n",
    "            for j in range(n):            \n",
    "                if i != j:\n",
    "                    sum += A[i,j] * x[j]\n",
    "            x[i] = (b[i] - sum)/A[i,i]\n",
    "        \n",
    "        ferror = np.linalg.norm(x - correct_solution, np.inf)\n",
    "        berror = np.linalg.norm(b - np.dot(A, x), np.inf)\n",
    "\n",
    "        if ferror < eps or (max_iterations is not None and k >= max_iterations):\n",
    "            break\n",
    "        \n",
    "        k+= 1\n",
    "\n",
    "    return x, k, ferror, berror\n",
    "\n",
    "x_0 = np.zeros(n)\n",
    "\n",
    "x, iterations, ferror, berror = Gauss_Seidel(A, b, x0)\n",
    "\n",
    "#print('A = \\n', A)\n",
    "#print()\n",
    "#print('x = \\n', x)\n",
    "print('With Gauss_Seidel Method:')\n",
    "print()\n",
    "print('Iterations:', iterations)\n",
    "print()\n",
    "print('Forward error:', ferror)\n",
    "print()\n",
    "print('Backward error:', berror)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31d3e025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With SOR Method when \\omega = 1.2\n",
      "\n",
      "Iterations: 15\n",
      "\n",
      "Forward error: 4.0626615649408393e-07\n",
      "\n",
      "Backward error: 1.5515907683116836e-06\n"
     ]
    }
   ],
   "source": [
    "# SOR method, omega = 1.2\n",
    "\n",
    "def SOR(A, b, x0, omega, eps=1e-6, max_iterations=None):\n",
    "    n = A.shape[0]\n",
    "    x = x_0.copy()\n",
    "    k = 0\n",
    "    \n",
    "    while k < n:\n",
    "        x_2 = x.copy()\n",
    "        for i in range(n):\n",
    "            sum = 0\n",
    "            for j in range(n):\n",
    "                if i != j:\n",
    "                    sum += A[i,j]*x_2[j]\n",
    "            x_2[i] = omega*(b[i] - sum)/A[i,i] + (1.0 - omega) * x[i]\n",
    "        ferror = np.linalg.norm(x_2 - correct_solution, np.inf)\n",
    "        berror = np.linalg.norm(b - np.dot(A, x_2), np.inf)\n",
    "        if ferror < eps or (max_iterations is not None and k >= max_iterations):\n",
    "            break\n",
    "        x = x_2\n",
    "        k += 1\n",
    "    return x, k, ferror, berror\n",
    "\n",
    "x_0 = np.zeros(n)\n",
    "\n",
    "omega = 1.2\n",
    "\n",
    "x, iterations, ferror, berror = SOR(A, b, x0, omega)\n",
    "\n",
    "#print('A = \\n', A)\n",
    "#print()\n",
    "#print('x = \\n', x)\n",
    "#print()\n",
    "print('With SOR Method when \\omega = 1.2')\n",
    "print()\n",
    "print('Iterations:', iterations)\n",
    "print()\n",
    "print('Forward error:', ferror)\n",
    "print()\n",
    "print('Backward error:', berror)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8f770b",
   "metadata": {},
   "source": [
    "# Problem 3\n",
    "Solve the system $Hx = b$ by the Conjugate Gradient Method, where $H$ is the $n \\times n$ Hilbert matrix (see Wikipedia for definition) and $b$ is the vector of all ones, for \n",
    "\n",
    "(a) $n = 4$\n",
    "\n",
    "(b) $n = 8$.\n",
    "\n",
    "What can you say about the solutions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c49053eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  -4.,   60., -180.,  140.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy as sp\n",
    "\n",
    "n = 4\n",
    "H = sp.linalg.hilbert(n)\n",
    "b = np.ones(n)\n",
    "x0 = np.zeros(n)\n",
    "\n",
    "def ConjGrad(A, b, x0, eps):    \n",
    "    if not np.array_equal(A.T, A):\n",
    "        print('Error: the matrix A is not symmetric!')\n",
    "        return\n",
    "        \n",
    "    n = A.shape[0]\n",
    "    r = np.zeros((n,n+1)) \n",
    "    u = np.zeros((n,n+1)) \n",
    "    r[:,0] = b-np.dot(A, x0)\n",
    "    if np.linalg.norm(r) < eps:\n",
    "        return x0\n",
    "    u[:,0] = r[:,0]\n",
    "    x = x0.copy()\n",
    "    \n",
    "    for k in range(n):        \n",
    "        a = np.dot(u[:,k], r[:,k])/np.dot(u[:,k], np.dot(A,u[:,k]))\n",
    "        x += a*u[:,k]\n",
    "        r[:,k+1] = b - np.dot(A, x)\n",
    "        if np.linalg.norm(r) < eps:\n",
    "            return x\n",
    "        sum = np.zeros(n)\n",
    "        for i in range(k+1):\n",
    "            sum += np.dot(r[:,k+1], np.dot(A, u[:,i]))/np.dot(u[:,i], np.dot(A, u[:,i]))*u[:,i]\n",
    "        u[:,k+1] = r[:,k+1] - sum\n",
    "        \n",
    "    return x\n",
    "\n",
    "ConjGrad(H, b, x0, 10e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94701251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-8.00000058e+00,  5.04000029e+02, -7.56000035e+03,  4.62000018e+04,\n",
       "       -1.38600005e+05,  2.16216006e+05, -1.68168004e+05,  5.14800012e+04])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 8\n",
    "H = sp.linalg.hilbert(n)\n",
    "b = np.ones(n)\n",
    "x0 = np.ones(n)\n",
    "\n",
    "ConjGrad(H, b, x0, 10e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ef24431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "n = 4\n",
      "\n",
      "Solution: (array([  -4.,   60., -180.,  140.]), 0)\n",
      "--------------------------------------------------\n",
      "n = 8\n",
      "\n",
      "Solution: (array([-7.99999699e+00,  5.03999798e+02, -7.55999759e+03,  4.61999879e+04,\n",
      "       -1.38599968e+05,  2.16215955e+05, -1.68167968e+05,  5.14799908e+04]), 0)\n"
     ]
    }
   ],
   "source": [
    "# checking answers\n",
    "\n",
    "n_values = [4, 8]\n",
    "\n",
    "for n in n_values:\n",
    "    H = sp.linalg.hilbert(n)\n",
    "    b = np.ones(n)\n",
    "    x = sp.sparse.linalg.cg(H, b)\n",
    "    \n",
    "    print('-'*50)\n",
    "    print('n =', n)\n",
    "    print()\n",
    "    print('Solution:', x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531702f2",
   "metadata": {},
   "source": [
    "As we would expect from the inherent ill-conditioning of the Hilbert matrix, the greater the number of entires (n), the less accurate the answers given to us by the Conjugate Gradient Method become. Meaning that it is possible that the Conjugate Gradient method may become impractical when a Hilbert matrix becomes large enough. \n",
    "\n",
    "To further illustrate this, we can compare the 2-norm condition numbers of several Hilbert matrices. Note how the rate of change increases slightly with each value of n (i.e. first step is scaled by a factor of ~19, second by ~27, third by ~29, fourth by ~31, etc.). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db5e73d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "n = 1\n",
      "\n",
      "Condition number for the Hilbert Matrix: 1.0\n",
      "------------------------------------------------------------\n",
      "n = 2\n",
      "\n",
      "Condition number for the Hilbert Matrix: 19.281470067903967\n",
      "------------------------------------------------------------\n",
      "n = 3\n",
      "\n",
      "Condition number for the Hilbert Matrix: 524.0567775860627\n",
      "------------------------------------------------------------\n",
      "n = 4\n",
      "\n",
      "Condition number for the Hilbert Matrix: 15513.738738929038\n",
      "------------------------------------------------------------\n",
      "n = 5\n",
      "\n",
      "Condition number for the Hilbert Matrix: 476607.25024100044\n",
      "------------------------------------------------------------\n",
      "n = 6\n",
      "\n",
      "Condition number for the Hilbert Matrix: 14951058.641453395\n",
      "------------------------------------------------------------\n",
      "n = 7\n",
      "\n",
      "Condition number for the Hilbert Matrix: 475367356.9114392\n",
      "------------------------------------------------------------\n",
      "n = 8\n",
      "\n",
      "Condition number for the Hilbert Matrix: 15257575566.627958\n"
     ]
    }
   ],
   "source": [
    "n_values = [1,2,3,4,5,6,7,8]\n",
    "\n",
    "for n in n_values:\n",
    "    H = sp.linalg.hilbert(n)\n",
    "    cond_num = np.linalg.cond(H)\n",
    "    \n",
    "    print('-'*60)\n",
    "    print('n =', n)\n",
    "    print()\n",
    "    print('Condition number for the Hilbert Matrix:', cond_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884f963e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a49c3d41",
   "metadata": {},
   "source": [
    "**MATH 5660 only:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc4dcb4",
   "metadata": {},
   "source": [
    "Convergence of iterative methods like the Conjugate Gradient Method can be accelerated\n",
    "by the use of a technique called **preconditioning**. The convergence rates of iterative\n",
    "methods often depend, directly or indirectly, on the condition number of the\n",
    "coefficient matrix A. The idea of preconditioning is to reduce the effective condition\n",
    "number of the problem.\n",
    "\n",
    "The preconditioned form of the $n \\times n$ linear system $A\\boldsymbol{x} = \\boldsymbol{b}$ is\n",
    "$$\n",
    "M^{-1}A\\boldsymbol{x} = M^{-1}\\boldsymbol{b},\n",
    "$$\n",
    "where $M$ is an invertible $n \\times n$ matrix called the **preconditioner**.\n",
    "\n",
    "When $A$ is a symmetric positive-definite $n \\times n$ matrix, we will choose a symmetric\n",
    "positive-definite matrix $M$ for use as a preconditioner. A particularly simple choice is the **Jacobi preconditioner** $M = D$, where $D$ is the diagonal of $A$. The Preconditioned Conjugate Gradient\n",
    "Method is now easy to describe: Replace $A\\boldsymbol{x} = \\boldsymbol{b}$ with the preconditioned equation $M^{−1} A\\boldsymbol{x} = M^{−1}\\boldsymbol{b}$, and replace the Euclidean inner product with $(\\boldsymbol{v},\\boldsymbol{w})M$.\n",
    "\n",
    "To convert Algorithm 2 in Section 3.3 to the preconditioned version, let $\\boldsymbol{z}_k = M^{-1}\\boldsymbol{b} - M^{-1}A\\boldsymbol{x}_k = M^{-1}\\boldsymbol{r}_k$. Then the algorithm is\n",
    "\n",
    "1. Initialize $\\boldsymbol{x}_0$ as any vector. Set $\\boldsymbol{r}_0=\\boldsymbol{b}-A\\boldsymbol{x}_0$ and $\\boldsymbol{u}_0=\\boldsymbol{z}_0=M^{-1}\\boldsymbol{r}_0$.\n",
    "\n",
    "2. For $k=0, 1, \\dots, n-1$:\n",
    "\n",
    "    1. $a_k=\\frac{\\boldsymbol{r}_k^T \\boldsymbol{z}_k}{\\boldsymbol{u}_k^TA\\boldsymbol{u}_k}$\n",
    "    2. $\\boldsymbol{x}_{k+1} = \\boldsymbol{x}_k + a_k \\boldsymbol{u}_k$\n",
    "    3. $\\boldsymbol{r}_{k+1} = \\boldsymbol{r}_{k}-a_kA\\boldsymbol{u}_{k}$\n",
    "    4. if ($||\\boldsymbol{r}_{k+1}|| < \\epsilon$):\n",
    "        1. break\n",
    "    5. $\\boldsymbol{z}_{k+1} = M^{-1}\\boldsymbol{r}_{k+1}$\n",
    "    6. $\\boldsymbol{b}_k = \\frac{\\boldsymbol{r}_{k+1}^T\\boldsymbol{z}_{k+1}}{\\boldsymbol{r}_{k}^T\\boldsymbol{z}_{k}}$\n",
    "    7. $\\boldsymbol{u}_{k+1} = \\boldsymbol{z}_{k+1} + \\boldsymbol{b}_k\\boldsymbol{u}_{k}$\n",
    "\n",
    "3. return $\\boldsymbol{x}_{k+1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926388d1",
   "metadata": {},
   "source": [
    "Now, consider the following problem.\n",
    "\n",
    "Let $A$ be the $n \\times n$ matrix with $n = 1000$ and entries $A(i, i) = i$, $A(i, i+1) = A(i+1, i) = 1/2$, $A(i, i + 2) = A(i + 2, i ) = 1/2$ for all $i$ that fit\n",
    "within the matrix. \n",
    "\n",
    "(a) Take a look at the nonzero structure of the matrix using plt.spy(A). \n",
    "\n",
    "(b) Let $\\boldsymbol{x}_e$ be the vector of $n$ ones (exact solution). Set $\\boldsymbol{b} = A\\boldsymbol{x}_e$, and apply the Conjugate Gradient Method, without preconditioner, and\n",
    "with the Jacobi preconditioner. Compare errors (using 2-norm) of the two runs in a plot versus step number (using semilogy). (So you need to modify the conjugate gradient codes to keep track of and return the solutions of all steps.) Use eps = 1e-10. \n",
    "\n",
    "The two methods may converge in different number of steps. Which one do you see is faster?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
